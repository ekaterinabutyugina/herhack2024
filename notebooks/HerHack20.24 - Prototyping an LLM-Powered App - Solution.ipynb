{"cells":[{"cell_type":"markdown","metadata":{"id":"vWmc_s2ezvU0"},"source":["<center><a target=\"_blank\" href=\"https://academy.constructor.org/\"><img src=https://lh3.googleusercontent.com/d/1EmH3Jks5CpJy0zK3JbkvJZkeqWtVcxhB width=\"500\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n","<hr />\n","\n","# <h1 align=\"center\"> HerHack20.24 - Prototyping an LLM-Powered App </h1>\n","\n","<hr />\n","<center>Constructor Academy, 2024</center>\n","\n","We are going to build a simple File QA chatbot to chat with your files using the  **[OpenAI API](https://openai.com/)** and **[Streamlit](https://streamlit.io/generative-ai)**.\n","\n","This Streamlit app allows users to upload text articles and ask questions about the content of these articles. It uses OpenAI's GPT model to generate answers, providing an interactive way to explore information within documents. 汳ｬ\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Install dependencies\n","- `streamlit` is needed to build and deploy interactive web applications with Python, ideal for integrating machine learning models and natural language processing (NLP) tasks.\n","- `openai` is needed for accessing and integrating OpenAI models like GPT-3.5 into the app for tasks such as text generation, summarization, and more."],"metadata":{"id":"jxulMlRxstDy"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7407,"status":"ok","timestamp":1728935491216,"user":{"displayName":"Ekaterina Butyugina","userId":"10324182844490961884"},"user_tz":-120},"id":"RvlYkCQ9vFiy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf56c0c2-b752-4698-9c62-23cc43c4601b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q streamlit"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8763,"status":"ok","timestamp":1728935499977,"user":{"displayName":"Ekaterina Butyugina","userId":"10324182844490961884"},"user_tz":-120},"id":"Q_UVJcjyUVSL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cfcb5858-1b3d-4b49-e692-5648e2614024"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/383.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m \u001b[32m378.9/383.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q openai"]},{"cell_type":"markdown","metadata":{"id":"waCfwniZOow8"},"source":["## Create a streamlit app minimal example\n"]},{"cell_type":"markdown","metadata":{"id":"meJ36PefNftd"},"source":["```python\n","%%writefile app.py\n","\n","import streamlit as st\n","\n","st.title(\"Hello World\")\n","\n"]},{"cell_type":"code","source":["## Write code here\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UoIVdbTetxWS","executionInfo":{"status":"ok","timestamp":1728928565395,"user_tz":-120,"elapsed":419,"user":{"displayName":"Ekaterina Butyugina","userId":"10324182844490961884"}},"outputId":"9c5055a1-5fb1-4536-aa4b-9e84c457c551"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"c-zloq7JuO4x"},"source":["## Install localtunnel\n","\n","- `localtunnel` - LocalTunnel is a tool that allows you to share your locally running Streamlit app with others over the web. It creates a publicly accessible URL for your locally hosted Streamlit server, making it easier to collaborate and showcase your app without deploying it to a cloud service. It is very convenient to test your app also."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2637,"status":"ok","timestamp":1728935387598,"user":{"displayName":"Ekaterina Butyugina","userId":"10324182844490961884"},"user_tz":-120},"outputId":"58d93b14-73ce-4e6f-a8d4-45b72280d130","id":"-eQ2eEOJuO4y"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K\u001b[?25h\n","added 22 packages, and audited 23 packages in 2s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n","\n","To address all issues, run:\n","  npm audit fix\n","\n","Run `npm audit` for details.\n"]}],"source":["!npm install localtunnel"]},{"cell_type":"markdown","metadata":{"id":"EB0vmMyMuO4y"},"source":["## Run streamlit in background"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vH2bFx3NuO4y"},"outputs":[],"source":["!streamlit run /content/app.py &>/content/logs.txt &"]},{"cell_type":"markdown","source":["### Explanations:\n","\n","1. `!streamlit run /content/app.py`:\n","\n","This runs a Streamlit app located at `/content/app.py` path.\n","\n","2. `&>/content/logs.txt`:\n","\n","The `&>` redirects both standard output and standard error of the command to the file /content/logs.txt. This means all logs (including errors) generated by the Streamlit app will be written to this file.\n","\n","3. `&`:\n","\n","The `&` at the end puts the command in the background, allowing the terminal or script to continue executing other commands while the Streamlit app runs."],"metadata":{"id":"VbKwpHNIJw3d"}},{"cell_type":"markdown","metadata":{"id":"XcLixoGOuO4y"},"source":["## Expose the port 8501\n","Then just click in the `url` showed.\n","\n","A `log.txt`file will be created.  \n","\n","**Note:** The pop-up will ask for password: just copy an IP address which is shown above your URL, and paste it to the password field.   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3166093b-0cfe-40f1-8a6b-ac1e4ee336f9","id":"F1AkvXpLuO4z","executionInfo":{"status":"ok","timestamp":1728932562403,"user_tz":-120,"elapsed":2727729,"user":{"displayName":"Ekaterina Butyugina","userId":"10324182844490961884"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["34.147.1.180\n","your url is: https://forty-roses-stand.loca.lt\n","/content/node_modules/localtunnel/bin/lt.js:81\n","    throw err;\n","    ^\n","\n","Error: connection refused: localtunnel.me:34939 (check your firewall settings)\n","    at Socket.<anonymous> \u001b[90m(/content/\u001b[39mnode_modules/\u001b[4mlocaltunnel\u001b[24m/lib/TunnelCluster.js:52:11\u001b[90m)\u001b[39m\n","\u001b[90m    at Socket.emit (node:events:513:28)\u001b[39m\n","\u001b[90m    at emitErrorNT (node:internal/streams/destroy:157:8)\u001b[39m\n","\u001b[90m    at emitErrorCloseNT (node:internal/streams/destroy:122:3)\u001b[39m\n","\u001b[90m    at processTicksAndRejections (node:internal/process/task_queues:83:21)\u001b[39m\n"]}],"source":["!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"]},{"cell_type":"markdown","source":["### Explanations:\n","\n","1. `!npx localtunnel --port 8501 &:`\n","\n","- The `npx localtunnel --port 8501` command uses LocalTunnel to create a public URL that redirects to a locally running web server (in this case, on port 8501, which is commonly used for Streamlit apps).\n","- The & at the end runs this process in the background, allowing the terminal to continue with the next command while LocalTunnel runs.\n","\n","2. `curl ipv4.icanhazip.com`:\n","\n","- `curl` is a tool to make network requests. Here, it is used to request the IP address of the machine running this script by querying the service ipv4.icanhazip.com, which simply returns the external IP address of the system making the request.\n","\n","In summary, this line of code first starts LocalTunnel to expose a local web server to the internet, and then retrieves the public IP address of the machine running the code.\n","\n","We'll use this public IP address in the next step as a password."],"metadata":{"id":"-0qR06l7JLkj"}},{"cell_type":"markdown","source":["## Create a streamlit QA application\n","\n","```python\n","%%writefile app.py\n","\n","import streamlit as st  \n","from openai import OpenAI\n","\n","# Sidebar section to capture the OpenAI API key\n","with st.sidebar:\n","  openai_api_key = ''  # Placeholder for the OpenAI API key\n","  if openai_api_key:\n","      st.text(\"OpenAI API Key provided\")  # Display a message if the API key is provided\n","  else:\n","      # Input field to capture the OpenAI API key securely as a password\n","      openai_api_key = st.text_input(\"OpenAI API Key\", key=\"chatbot_api_key\", type=\"password\")\n","      \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n","\n","# Main title of the app\n","st.title(\"沒 File Q&A with ChatGPT\")\n","\n","# File uploader to allow users to upload text or markdown articles\n","uploaded_file = st.file_uploader(\"Upload an article\", type=(\"txt\", \"md\"))\n","\n","# Text input for users to ask a question related to the uploaded article\n","question = st.text_input(\n","    \"Ask something about the article\",\n","    placeholder=\"Can you give me a short summary?\",  \n","    disabled=not uploaded_file,\n",")\n","\n","# Inform the user to provide an OpenAI API key if file and question are provided but no key is entered\n","if uploaded_file and question and not openai_api_key:\n","    st.info(\"Please add your OpenaI API key to continue.\")\n","\n","# If both the file, question, and API key are provided, process the input\n","if uploaded_file and question and openai_api_key:\n","    # Read the uploaded article and decode it to a string\n","    article = uploaded_file.read().decode()\n","\n","    # Create a prompt combining the article content and the user's question\n","    my_prompt = f\"\"\"Here's an article:{article}.\\n\\n\n","    \\n\\n\\n\\n{question}\"\"\"\n","\n","    # Initialize the OpenAI client with the provided API key\n","    client = OpenAI(api_key=openai_api_key)\n","\n","    # Prepare messages for the OpenAI chat model, including a system message and the user prompt\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # System message to set the assistant's behavior\n","        {\"role\": \"user\", \"content\": my_prompt}  # User message with the article and question\n","    ]\n","\n","    # Send the message to the GPT-3.5 model for generating a response\n","    response = client.chat.completions.create(\n","         model=\"gpt-3.5-turbo\",  \n","         messages=messages  \n","     )\n","\n","    # Display the answer generated by the model\n","    st.write(\"### Answer\")\n","    st.write(response.choices[0].message.content)  \n","\n"],"metadata":{"id":"U4EdztgxtgHP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"m60OXsfaG8uv"},"outputs":[],"source":["##  change the app.py file\n"]},{"cell_type":"markdown","source":["## Exercise:\n","\n","### Improving File Q&A ChatBot using OpenAI API\n","\n","- Build your own app and share the link\n","- Increase the length of the Answer by checking parameters [here](https://platform.openai.com/docs/api-reference/chat/create).\n","- Add functionality of parsing PDF files. Check the documentation [here](https://pdfminersix.readthedocs.io/en/latest/tutorial/composable.html).\n","\n","- **Bonus**: deploy your code using GitHub and Streamlit cloud by following [these instructions](https://github.com/ekaterinabutyugina/fileqabot_openai). Be mindful of the API KEY, don窶冲 let any of those go to the public resources.  \n"],"metadata":{"id":"ipAFeScFh5JE"}},{"cell_type":"markdown","source":["## Solution"],"metadata":{"id":"DQbOMJXvkwC9"}},{"cell_type":"code","source":["!pip install -q pdfminer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iyj0nedkzsK","executionInfo":{"status":"ok","timestamp":1728935365034,"user_tz":-120,"elapsed":32821,"user":{"displayName":"Ekaterina Butyugina","userId":"10324182844490961884"}},"outputId":"3d98f373-51ff-48a9-f3a8-7eb8dcf414ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/4.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤\u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m0.3/4.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m2.7/4.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["## Create a streamlit QA application with PDF parsing\n","\n","```python\n","%%writefile qa_pdf_app.py\n","\n","import streamlit as st\n","from openai import OpenAI\n","from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n","from pdfminer.converter import TextConverter\n","from pdfminer.layout import LAParams\n","from pdfminer.pdfpage import PDFPage\n","from io import StringIO\n","\n","\n","# Open Sidebar\n","with st.sidebar:\n","  openai_api_key = ''  # Placeholder for the OpenAI API key\n","  if openai_api_key:\n","      st.text(\"OpenAI API Key provided\")\n","  else:\n","    openai_api_key = st.text_input(\"OpenAI API Key\", key=\"chatbot_api_key\", type=\"password\")\n","    # adding a hyperlink\n","    \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n","\n","# Set Title:\n","st.title(\"沒 File Q&A with ChatGPT\")\n","\n","# Upload the file:\n","uploaded_file = st.file_uploader(\"Upload an article\", type=(\"txt\", \"md\", \"pdf\"))\n","\n","# Text input:\n","question = st.text_input(\n","    \"Ask something about the article\",\n","    placeholder=\"Can you give me a short summary?\",\n","    disabled=not uploaded_file,\n",")\n","\n","if uploaded_file and question and not openai_api_key:\n","    st.info(\"Please add your Anthropic API key to continue.\")\n","\n","if uploaded_file and question and openai_api_key:\n","    if uploaded_file.name.split(\".\")[-1] == \"pdf\":\n","      rsrcmgr = PDFResourceManager()\n","      retstr = StringIO()\n","      laparams = LAParams()\n","      device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n","      interpreter = PDFPageInterpreter(rsrcmgr, device)\n","      file_pages = PDFPage.get_pages(uploaded_file,check_extractable=False)\n","      for page in file_pages:\n","        interpreter.process_page(page)\n","\n","      article = retstr.getvalue()\n","      device.close()\n","      retstr.close()\n","      # st.write(article)\n","\n","    else:\n","      article = uploaded_file.read().decode()\n","      # st.write(article)\n","\n","    # Prompting\n","    my_prompt = f\"\"\"Here's an article:{article}.\\n\\n\n","    \\n\\n\\n\\n{question}\"\"\"\n","\n","    # Initialize the OpenAI client with the provided API key\n","    client = OpenAI(api_key=openai_api_key)\n","\n","    # Prepare messages for the OpenAI chat model, including a system message and the user prompt\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # System message to set the assistant's behavior\n","        {\"role\": \"user\", \"content\": my_prompt}  # User message with the article and question\n","    ]\n","\n","    # Send the message to the GPT-3.5 model for generating a response\n","    response = client.chat.completions.create(\n","         model=\"gpt-3.5-turbo\",  \n","         messages=messages,\n","         max_tokens=200,  \n","     )\n","\n","    # Display the answer generated by the model\n","    st.write(\"### Answer\")\n","    st.write(response.choices[0].message.content)\n","```"],"metadata":{"id":"mkMMh9oOjhJG"}},{"cell_type":"code","source":["##  change the app.py file\n"],"metadata":{"id":"konL0WWnnhWS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!streamlit run /content/qa_pdf_app.py &>/content/logs.txt &\n"],"metadata":{"id":"K01oUqELg8kg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nMNuNLrlLkf","executionInfo":{"status":"ok","timestamp":1728936011712,"user_tz":-120,"elapsed":485902,"user":{"displayName":"Ekaterina Butyugina","userId":"10324182844490961884"}},"outputId":"5dc400af-2efc-4ac7-c543-69975ef7ce60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34.134.93.107\n","your url is: https://bumpy-shirts-greet.loca.lt\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Vvuvi8WDlPoE"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Gn1wXn2uxORqKZYwwtTWOTuh_8wtTd5r","timestamp":1728657935976}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}